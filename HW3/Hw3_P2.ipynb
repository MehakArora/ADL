{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51974f10-9d90-441a-a37f-db695fa96958",
   "metadata": {},
   "source": [
    "### A lot of the code (evaluation) was taken from https://github.com/baotramduong/Twitter-Sentiment-Analysis-with-Deep-Learning-using-BERT/blob/main/Notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bccdc8-e99a-486b-80a9-2649cf2d89fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "device = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8351718c-7ec5-4bcd-b049-6e199086b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = imdb[\"train\"].shuffle(seed=42)\n",
    "test_dataset = imdb[\"test\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea3af4ba-5f50-4145-9a12-e9bf3aefbdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(np.array(train_dataset['label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e054fe-3155-4751-b5ad-d52a51fc9eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load pre-trained BERT\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize(samples):\n",
    "    return tokenizer(samples['text'], truncation=True)\n",
    "\n",
    "tokenized_imdb = imdb.map(tokenize, batched=True)\n",
    "     \n",
    "\n",
    "bert = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                      num_labels = num_classes,\n",
    "                                                      output_attentions = False,\n",
    "                                                      output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13645787-fc6d-4785-8cab-5ce50253327d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e37658-a6ea-41bf-ad25-923416b4559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, properly tokenize your dataset with explicit padding and truncation settings\n",
    "tokenized_imdb = {\n",
    "    'train': tokenizer(\n",
    "        imdb['train']['text'],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,  # Adjust this value based on your needs\n",
    "        return_tensors=None  # Important: don't convert to tensors yet\n",
    "    ),\n",
    "    'test': tokenizer(\n",
    "        imdb['test']['text'],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )\n",
    "}\n",
    "\n",
    "# Add labels to the tokenized datasets\n",
    "tokenized_imdb['train']['labels'] = imdb['train']['label']\n",
    "tokenized_imdb['test']['labels'] = imdb['test']['label']\n",
    "\n",
    "# Convert to Dataset objects if they aren't already\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(tokenized_imdb['train'])\n",
    "test_dataset = Dataset.from_dict(tokenized_imdb['test'])\n",
    "\n",
    "# Now create the DataLoader with the DataCollator\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create the data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Create PyTorch DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e918da77-b9e9-46c2-bd2d-2fa33ab6073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentimentClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, dropout = 0.2, input_dim = 768, classifier_dims = None):\n",
    "        super(sentimentClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if classifier_dims == None:\n",
    "            self.classifier_dims = [768, 768, 512, 512]\n",
    "        else:\n",
    "            self.classifier_dims = classifier_dims\n",
    "        \n",
    "        self.num_classifier_layers = len(self.classifier_dims)\n",
    "        \n",
    "        classifier_layers = []\n",
    "        for classifier_dim in self.classifier_dims:\n",
    "            classifier_layers.extend([\n",
    "                nn.Linear(input_dim, classifier_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(classifier_dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            input_dim = classifier_dim\n",
    "        \n",
    "        classifier_layers.append(nn.Linear(classifier_dim, 2))\n",
    "        self.classifier = nn.Sequential(*classifier_layers)\n",
    "        self.freeze_pretrained()\n",
    "    \n",
    "    def freeze_pretrained(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        encoded = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = self.dropout(encoded[1])\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "\n",
    "def analyze_state_dict_shapes_and_names(model):\n",
    "    \"\"\"Analyze model's state dictionary\"\"\"\n",
    "    print(\"\\n===== MODEL STATE DICT ANALYSIS =====\")\n",
    "    \n",
    "    # Get state dict\n",
    "    state_dict = model.state_dict()\n",
    "    \n",
    "    # Print keys and shapes\n",
    "    for name, param in state_dict.items():\n",
    "        print(f\"{name}: {param.shape}\")\n",
    "    \n",
    "    # Check trainable vs. non-trainable parameters\n",
    "    trainable_params = {name: param for name, param in model.named_parameters() if param.requires_grad}\n",
    "    non_trainable_params = {name: param for name, param in model.named_parameters() if not param.requires_grad}\n",
    "    \n",
    "    print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params.values())}\")\n",
    "    print(f\"Non-trainable parameters: {sum(p.numel() for p in non_trainable_params.values())}\")\n",
    "    \n",
    "    # Check for any parameters that aren't trainable\n",
    "    #if non_trainable_params:\n",
    "    #    print(\"\\nNon-trainable parameter names:\")\n",
    "    #    for name in non_trainable_params.keys():\n",
    "    #        print(f\"- {name}\")\n",
    "    #else:\n",
    "    #    print(\"\\nAll parameters are trainable\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff62a5b3-e23f-41aa-8f97-739fe846db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sentimentClassifier(bert_model = bert.bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "244b717b-a3a6-47a5-8022-32bbb76940b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== MODEL STATE DICT ANALYSIS =====\n",
      "bert.embeddings.word_embeddings.weight: torch.Size([30522, 768])\n",
      "bert.embeddings.position_embeddings.weight: torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight: torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight: torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.pooler.dense.weight: torch.Size([768, 768])\n",
      "bert.pooler.dense.bias: torch.Size([768])\n",
      "classifier.0.weight: torch.Size([768, 768])\n",
      "classifier.0.bias: torch.Size([768])\n",
      "classifier.2.weight: torch.Size([768])\n",
      "classifier.2.bias: torch.Size([768])\n",
      "classifier.2.running_mean: torch.Size([768])\n",
      "classifier.2.running_var: torch.Size([768])\n",
      "classifier.2.num_batches_tracked: torch.Size([])\n",
      "classifier.4.weight: torch.Size([768, 768])\n",
      "classifier.4.bias: torch.Size([768])\n",
      "classifier.6.weight: torch.Size([768])\n",
      "classifier.6.bias: torch.Size([768])\n",
      "classifier.6.running_mean: torch.Size([768])\n",
      "classifier.6.running_var: torch.Size([768])\n",
      "classifier.6.num_batches_tracked: torch.Size([])\n",
      "classifier.8.weight: torch.Size([512, 768])\n",
      "classifier.8.bias: torch.Size([512])\n",
      "classifier.10.weight: torch.Size([512])\n",
      "classifier.10.bias: torch.Size([512])\n",
      "classifier.10.running_mean: torch.Size([512])\n",
      "classifier.10.running_var: torch.Size([512])\n",
      "classifier.10.num_batches_tracked: torch.Size([])\n",
      "classifier.12.weight: torch.Size([512, 512])\n",
      "classifier.12.bias: torch.Size([512])\n",
      "classifier.14.weight: torch.Size([512])\n",
      "classifier.14.bias: torch.Size([512])\n",
      "classifier.14.running_mean: torch.Size([512])\n",
      "classifier.14.running_var: torch.Size([512])\n",
      "classifier.14.num_batches_tracked: torch.Size([])\n",
      "classifier.16.weight: torch.Size([2, 512])\n",
      "classifier.16.bias: torch.Size([2])\n",
      "\n",
      "Total parameters: 111325954\n",
      "Trainable parameters: 1843714\n",
      "Non-trainable parameters: 109482240\n"
     ]
    }
   ],
   "source": [
    "analyze_state_dict_shapes_and_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9568425a-a513-4418-8c2d-d66dbc1b12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "output = model( batch['input_ids'], batch['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "351bb872-4964-4e19-91d8-a6399b08f244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/dctrl/ma618/torch/lib64/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "#load optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                 lr = 1e-5,\n",
    "                 eps = 1e-8)\n",
    "#load scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                           num_warmup_steps = 0,\n",
    "                                           num_training_steps = len(train_dataloader)*epochs)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average = 'weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    \n",
    "    #make prediction\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label}')\n",
    "        print(f'Accuracy:{len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "        \n",
    "def evaluate(validation_dataloader):\n",
    "\n",
    "    #evaluation mode disables the dropout layer \n",
    "    model.eval()\n",
    "    \n",
    "    #tracking variables\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_dataloader):\n",
    "\n",
    "            #load into GPU\n",
    "            batch = tuple(batch[b].to(device) for b in batch.keys())\n",
    "\n",
    "            #define inputs\n",
    "            inputs = {'input_ids': batch[0],\n",
    "                      'attention_mask': batch[1]}\n",
    "            labels = batch[2]\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_val_total += loss.item()\n",
    "\n",
    "            #compute accuracy\n",
    "            logits = outputs.detach().cpu().numpy()\n",
    "            label_ids = labels.cpu().numpy()\n",
    "            predictions.append(logits)\n",
    "            true_vals.append(label_ids)\n",
    "    \n",
    "    #compute average loss\n",
    "    loss_val_avg = loss_val_total/len(validation_dataloader) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65709db2-f224-4e24-abbc-647dbc383657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm \n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ce56cdd-2da0-4fd3-ad32-7a3bdecf9417",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [05:31<00:00,  4.72it/s]\n",
      "100%|██████████| 1563/1563 [05:14<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Loss: 0.6789 - Val Accuracy: 0.7266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [05:30<00:00,  4.72it/s]\n",
      "100%|██████████| 1563/1563 [05:19<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - Loss: 0.6472 - Val Accuracy: 0.7473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [05:15<00:00,  4.96it/s]\n",
      "100%|██████████| 1563/1563 [04:16<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - Loss: 0.6335 - Val Accuracy: 0.7602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [04:31<00:00,  5.76it/s]\n",
      "100%|██████████| 1563/1563 [04:16<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - Loss: 0.6155 - Val Accuracy: 0.7630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [04:31<00:00,  5.75it/s]\n",
      "100%|██████████| 1563/1563 [04:16<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - Loss: 0.6096 - Val Accuracy: 0.7703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [04:31<00:00,  5.76it/s]\n",
      "100%|██████████| 1563/1563 [04:26<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - Loss: 0.5952 - Val Accuracy: 0.7760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [04:31<00:00,  5.76it/s]\n",
      "100%|██████████| 1563/1563 [04:16<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - Loss: 0.5932 - Val Accuracy: 0.7743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [04:32<00:00,  5.74it/s]\n",
      "100%|██████████| 1563/1563 [04:16<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - Loss: 0.5888 - Val Accuracy: 0.7777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [04:31<00:00,  5.75it/s]\n",
      "100%|██████████| 1563/1563 [04:16<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - Loss: 0.5889 - Val Accuracy: 0.7737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [04:31<00:00,  5.76it/s]\n",
      "100%|██████████| 1563/1563 [04:16<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - Loss: 0.5887 - Val Accuracy: 0.7721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [04:31<00:00,  5.76it/s]\n",
      "100%|██████████| 1563/1563 [04:16<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - Loss: 0.5887 - Val Accuracy: 0.7743\n",
      "Early stopping after 11 epochs\n"
     ]
    }
   ],
   "source": [
    "# Early stopping setup\n",
    "best_val = 0\n",
    "patience = 3\n",
    "counter = 0\n",
    "    \n",
    "# Training history\n",
    "history = {\n",
    "    'accuracy': [],\n",
    "    'val_accuracy': []\n",
    "}\n",
    "    \n",
    "# Training loop\n",
    "epochs = 25\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    true_vals = []\n",
    "    loss_train_total = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        #set gradient to 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = tuple(batch[b].to(device) for b in batch.keys())\n",
    "\n",
    "        #define inputs\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1]}\n",
    "        labels = batch[2]\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_train_total +=loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        \n",
    "        #update optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        #update scheduler\n",
    "        scheduler.step()        \n",
    "    #print training result\n",
    "    loss_train_avg = loss_train_total/len(train_dataloader)    \n",
    "    \n",
    "    #evaluate\n",
    "    val_loss, predictions, true_vals = evaluate(validation_dataloader)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Loss: {loss_train_avg:.4f} - Val Accuracy: {val_f1:.4f}')\n",
    "    \n",
    "    if val_f1 > best_val:\n",
    "            best_val = val_f1\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_model_sentiment.pt')\n",
    "            counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs')\n",
    "            # Load best model\n",
    "            model.load_state_dict(torch.load('best_model_sentiment.pt'))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad3c5f0-40bc-4a84-8616-50478ce0af25",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68877a83-e57a-4d83-9b63-0fe173f379f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [04:18<00:00,  6.04it/s]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_sentiment.pt'))\n",
    "val_loss, predictions, true_vals = evaluate(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71448bd2-63a1-44c4-af0a-0c11b0dc5122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5036351951417142\n",
      "F1 Score (weighted): 0.7777134890738397\n"
     ]
    }
   ],
   "source": [
    "val_f1 = f1_score_func(predictions, true_vals)\n",
    "print(f'Validation loss: {val_loss}')\n",
    "print(f'F1 Score (weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c4157cb-8eb9-40f5-b28b-4a4707d769db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0\n",
      "Accuracy:9343/12500\n",
      "\n",
      "Class: 1\n",
      "Accuracy:10105/12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877344a3-c445-4479-9620-b31e8ce73db3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
